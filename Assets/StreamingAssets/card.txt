A,画像や文章の特定の部分に注意を向けるよう、学習させていく方法
B,TransformerのEncoderを使ったモデルで事前学習としてMLMとNSPをする
C,OpenAI社が開発したチャットボット
D,テキストから画像生成するように学習した、GPT3の120Bパラメータ版
E,2層のLSTMをベースとしたモデルで、文脈に応じた単語の意味を表すことが可能
F,sequence2sequenceモデル特化の深層学習ライブラリ
G,言語理解タスクの精度を評価するためのベンチマーク
H,指定された変数からのデータ間の距離を定義し、その距離を使用してデータをクラスタに分類する
I,深層学習における、データの入力部分
J,PythonやRなどの開発環境
K,特徴空間における最も近い訓練データに基づく分類手法
L,長期的特徴と短期的特徴を学習することができる
M,複数のアテンションヘッドを並列実行して，系列中の各トークン表現の変換を行う
N,入力された2文に意味的につながりがあるかどうかを予測する
O,最適化アルゴリズム
P,単語の順序に関する情報が埋め込まれたベクトル
Q,数値的に測定できない定性的な変数
R,再帰構造により系列データを学習可能にするニューラルネットワークの総称
S,複数の出力値の合計が1になるように変換して出力する関数
T,Attentionのみを使用したEncoder　Decoderモデル
U,学習データに正解を与えない状態で学習させる学習手法
V,作成した機械学習予測モデルの精度を検証する手法
W,単語の埋め込み表現を計算するモデル
X,予測対象の単語同士の依存関係を学習できる自己回帰言語モデル
Y,yだけありません
Z,モデルにタスクの例を1つも与えない手法